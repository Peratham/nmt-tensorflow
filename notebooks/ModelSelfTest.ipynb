{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from translate.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def self_test_model():\n",
    "    \"\"\"\n",
    "    Test the translation model.\n",
    "    \"\"\"\n",
    "    def linebreak():\n",
    "        print('-' * 50)\n",
    "    \n",
    "    print(\"Self-test for neural translation model.\")\n",
    "    linebreak()\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        with tf.device('/cpu:0'):\n",
    "            t = time()\n",
    "            # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "            model = Model(source_vocab_size=10, \n",
    "                          target_vocab_size=10,\n",
    "                          buckets=[(3, 3), (6, 6)], \n",
    "                          size=32,\n",
    "                          num_layers=2,\n",
    "                          learning_rate=None,\n",
    "                          max_gradient_norm=5.0, \n",
    "                          batch_size=32,\n",
    "                          use_lstm=True, \n",
    "                          use_local=True,\n",
    "                          optim='adam',\n",
    "                          num_samples=None)\n",
    "            \n",
    "            print(\"Initializing Model took %.6fs\" %(time() - t))\n",
    "            linebreak()\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        t = time()\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        print(\"Initializing Variables took %.6fs\" %(time() - t))\n",
    "        linebreak()\n",
    "\n",
    "        # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
    "        data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "                    [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "        num_iter = 20\n",
    "        \n",
    "        print('Using Learning Rate: %.2f' %(model.learning_rate.eval()))\n",
    "        linebreak()\n",
    "        \n",
    "        t = time()\n",
    "        # Train the fake model for 5 steps.\n",
    "        for _ in xrange(num_iter):\n",
    "            bucket_id = random.choice([0, 1])\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "            loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "            print('Perplexity: %f' %(np.exp(loss)))\n",
    "        linebreak()\n",
    "        print(\"Average training time: %.6fs/iter\" %((time() - t)/num_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-test for neural translation model.\n",
      "--------------------------------------------------\n",
      "Initializing Model took 8.957379s\n",
      "--------------------------------------------------\n",
      "Initializing Variables took 4.395649s\n",
      "--------------------------------------------------\n",
      "Using Learning Rate: 0.01\n",
      "--------------------------------------------------\n",
      "Perplexity: 5.452719\n",
      "Perplexity: 3.408495\n",
      "Perplexity: 2.830749\n",
      "Perplexity: 3.462721\n",
      "Perplexity: 1.738196\n",
      "Perplexity: 4.457092\n",
      "Perplexity: 3.844059\n",
      "Perplexity: 2.823817\n",
      "Perplexity: 1.789566\n",
      "Perplexity: 1.629817\n",
      "Perplexity: 1.521011\n",
      "Perplexity: 1.398613\n",
      "Perplexity: 1.226592\n",
      "Perplexity: 1.213510\n",
      "Perplexity: 2.402075\n",
      "Perplexity: 2.415878\n",
      "Perplexity: 1.075651\n",
      "Perplexity: 1.052696\n",
      "Perplexity: 1.694463\n",
      "Perplexity: 1.062993\n",
      "--------------------------------------------------\n",
      "Average training time: 0.032712s/iter\n"
     ]
    }
   ],
   "source": [
    "self_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def self_test_model_forward():\n",
    "    \"\"\"\n",
    "    Test the translation model.\n",
    "    \"\"\"\n",
    "    def linebreak():\n",
    "        print('-' * 50)\n",
    "    \n",
    "    print(\"Self-test for neural translation model.\")\n",
    "    linebreak()\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        with tf.device('/cpu:0'):\n",
    "            t = time()\n",
    "            # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "            model = Model(source_vocab_size=10, \n",
    "                          target_vocab_size=10,\n",
    "                          buckets=[(3, 3), (6, 6)], \n",
    "                          size=32,\n",
    "                          num_layers=2,\n",
    "                          learning_rate=None, \n",
    "                          batch_size=32, \n",
    "                          forward_only=True)\n",
    "            \n",
    "            print(\"Initializing Model took %.6fs\" %(time() - t))\n",
    "            linebreak()\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        t = time()\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        print(\"Initializing Variables took %.6fs\" %(time() - t))\n",
    "        linebreak()\n",
    "\n",
    "        # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
    "        data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "                    [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "        bucket_id = 1\n",
    "        \n",
    "        t = time()\n",
    "        # Test the output of the fake model.\n",
    "        encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "        output = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "        print(\"Model runtime was %.6fs\" %(time() - t))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-test for neural translation model.\n",
      "--------------------------------------------------\n",
      "Initializing Model took 1.762947s\n",
      "--------------------------------------------------\n",
      "Initializing Variables took 1.057347s\n",
      "--------------------------------------------------\n",
      "Model runtime was 0.047165s\n"
     ]
    }
   ],
   "source": [
    "output = self_test_model_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from translate.translate import get_data\n",
    "from translate.translate import _BUCKETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATADIR = '/Users/PragaashP/Desktop/f16/cs294-129/final-proj/datasets/dev/processed'\n",
    "EN_PATH = DATADIR + '/newstest2013.ids50000.en'\n",
    "FR_PATH = DATADIR + '/newstest2013.ids50000.fr'\n",
    "newstest2013 = get_data(en_ids_path=EN_PATH, fr_ids_path=FR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_weights(data):\n",
    "    std = np.sqrt(np.asarray(map(len, data)))\n",
    "    return std.cumsum()/std.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(data, checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Test the translation model.\n",
    "    \"\"\"\n",
    "    def linebreak():\n",
    "        print('-' * 50)\n",
    "    \n",
    "    print(\"Train neural translation model.\")\n",
    "    linebreak()\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        with tf.device('/cpu:0'):\n",
    "            t = time()\n",
    "            model = Model(source_vocab_size=50000, \n",
    "                          target_vocab_size=50000,\n",
    "                          buckets=_BUCKETS, \n",
    "                          size=128,\n",
    "                          num_layers=3,\n",
    "                          max_gradient_norm=5.0, \n",
    "                          batch_size=64,\n",
    "                          use_lstm=True,\n",
    "                          optim='adam',\n",
    "                          num_samples=None)\n",
    "            \n",
    "            print(\"Initializing Model took %.6fs\" %(time() - t))\n",
    "            linebreak()\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        t = time()\n",
    "        model.load(sess, checkpoint_dir)\n",
    "        print(\"Loading Model took %.6fs\" %(time() - t))\n",
    "        linebreak()\n",
    "\n",
    "        num_iter = 1001\n",
    "        checkpoint = 10\n",
    "        intervals = get_weights(data)\n",
    "        \n",
    "        print('Using Learning Rate: %.2f' %(model.learning_rate.eval()))\n",
    "        linebreak()\n",
    "        \n",
    "        t = time()\n",
    "        batch_loss, prev_losses = 0.0, []\n",
    "        for i in xrange(1, num_iter):\n",
    "            bucket_id = np.abs(np.random.rand() - intervals).argmin()\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(data, bucket_id)\n",
    "            loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "            batch_loss += (loss/checkpoint)\n",
    "            if i % checkpoint == 0:\n",
    "                if len(prev_losses) > 2 and batch_loss > max(prev_losses[-3:]):\n",
    "                    sess.run(model.learning_rate_decay_op)\n",
    "                prev_losses.append(batch_loss)\n",
    "                print('Perplexity: %f' %(np.exp(batch_loss)))\n",
    "                batch_loss = 0.0\n",
    "                \n",
    "        linebreak()\n",
    "        print(\"Average training time: %.6fs/iter\" %((time() - t)/num_iter))\n",
    "        model.save(sess, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train neural translation model.\n",
      "--------------------------------------------------\n",
      "Initializing Model took 93.722282s\n",
      "--------------------------------------------------\n",
      "Loading Model took 42.494763s\n",
      "--------------------------------------------------\n",
      "Using Learning Rate: 0.01\n",
      "--------------------------------------------------\n",
      "Perplexity: 6459.561820\n",
      "Perplexity: 1442.489431\n",
      "Perplexity: 886.798324\n",
      "Perplexity: 972.945839\n",
      "Perplexity: 798.610501\n",
      "Perplexity: 680.120646\n",
      "Perplexity: 498.986414\n",
      "Perplexity: 440.391930\n",
      "Perplexity: 434.633080\n",
      "Perplexity: 326.504664\n",
      "Perplexity: 404.900369\n",
      "Perplexity: 287.410460\n",
      "Perplexity: 375.540878\n",
      "Perplexity: 247.719276\n",
      "Perplexity: 272.915217\n",
      "Perplexity: 217.843840\n",
      "Perplexity: 287.530368\n",
      "Perplexity: 292.494319\n",
      "Perplexity: 178.639094\n",
      "Perplexity: 187.787048\n",
      "Perplexity: 185.398113\n",
      "Perplexity: 110.513220\n",
      "Perplexity: 105.202938\n",
      "Perplexity: 129.426412\n",
      "Perplexity: 56.826853\n",
      "Perplexity: 72.618852\n",
      "Perplexity: 121.749620\n",
      "Perplexity: 45.645900\n",
      "Perplexity: 123.780093\n",
      "Perplexity: 48.757051\n",
      "Perplexity: 24.918485\n",
      "Perplexity: 65.730801\n",
      "Perplexity: 47.026438\n",
      "Perplexity: 21.924936\n",
      "Perplexity: 52.458534\n",
      "Perplexity: 21.140516\n",
      "Perplexity: 56.829038\n",
      "Perplexity: 32.787855\n",
      "Perplexity: 21.251460\n",
      "Perplexity: 35.644106\n",
      "Perplexity: 33.687093\n",
      "Perplexity: 19.881217\n",
      "Perplexity: 25.648779\n",
      "Perplexity: 22.287602\n",
      "Perplexity: 10.340111\n",
      "Perplexity: 28.384553\n",
      "Perplexity: 21.054557\n",
      "Perplexity: 45.815924\n",
      "Perplexity: 10.893396\n",
      "Perplexity: 19.605466\n",
      "Perplexity: 37.996207\n",
      "Perplexity: 19.127312\n",
      "Perplexity: 7.200666\n",
      "Perplexity: 10.201709\n",
      "Perplexity: 19.269010\n",
      "Perplexity: 18.018275\n",
      "Perplexity: 23.369776\n",
      "Perplexity: 7.252042\n",
      "Perplexity: 3.692959\n",
      "Perplexity: 14.250697\n",
      "Perplexity: 8.622196\n",
      "Perplexity: 7.176473\n",
      "Perplexity: 16.904677\n",
      "Perplexity: 11.011318\n",
      "Perplexity: 33.418836\n",
      "Perplexity: 6.019005\n",
      "Perplexity: 6.027298\n",
      "Perplexity: 15.315069\n",
      "Perplexity: 8.493835\n",
      "Perplexity: 10.767700\n",
      "Perplexity: 8.568779\n",
      "Perplexity: 4.765149\n",
      "Perplexity: 8.863345\n",
      "Perplexity: 6.755313\n",
      "Perplexity: 9.091766\n",
      "Perplexity: 2.597205\n",
      "Perplexity: 3.096179\n",
      "Perplexity: 9.008842\n",
      "Perplexity: 6.319706\n",
      "Perplexity: 8.594518\n",
      "Perplexity: 6.506644\n",
      "Perplexity: 4.642692\n",
      "Perplexity: 3.009094\n",
      "Perplexity: 8.882156\n",
      "Perplexity: 5.380824\n",
      "Perplexity: 4.140142\n",
      "Perplexity: 4.699498\n",
      "Perplexity: 6.726110\n",
      "Perplexity: 5.502415\n",
      "Perplexity: 4.321007\n",
      "Perplexity: 10.040142\n",
      "Perplexity: 6.970238\n",
      "Perplexity: 2.636693\n",
      "Perplexity: 4.284763\n",
      "Perplexity: 9.536471\n",
      "Perplexity: 7.806389\n",
      "Perplexity: 2.915750\n",
      "Perplexity: 3.368271\n",
      "Perplexity: 2.808537\n",
      "Perplexity: 3.492950\n",
      "--------------------------------------------------\n",
      "Average training time: 2.909746s/iter\n"
     ]
    }
   ],
   "source": [
    "CKPT_DIR = '/Users/PragaashP/Desktop/f16/cs294-129/final-proj/nmt/checkpoints'\n",
    "train_model(newstest2013, CKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from translate.data_utils import get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_DIR = '/Users/PragaashP/Desktop/f16/cs294-129/final-proj/datasets/train/processed'\n",
    "fr_vocab, fr_rev_vocab = get_vocab(VOCAB_DIR, 50000, lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from translate.eval_utils import compute_corpus_bleu_score as bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_model(data, checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Test the translation model.\n",
    "    \"\"\"\n",
    "    def linebreak():\n",
    "        print('-' * 50)\n",
    "    \n",
    "    print(\"Test neural translation model.\")\n",
    "    linebreak()\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        with tf.device('/cpu:0'):\n",
    "            t = time()\n",
    "            model = Model(source_vocab_size=50000, \n",
    "                          target_vocab_size=50000,\n",
    "                          buckets=_BUCKETS, \n",
    "                          size=128,\n",
    "                          num_layers=3, \n",
    "                          batch_size=64,\n",
    "                          use_lstm=True,\n",
    "                          optim='adam',\n",
    "                          num_samples=None, \n",
    "                          forward_only=True)\n",
    "            \n",
    "            print(\"Initializing Model took %.6fs\" %(time() - t))\n",
    "            linebreak()\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        t = time()\n",
    "        model.load(sess, checkpoint_dir)\n",
    "        print(\"Loading Model took %.6fs\" %(time() - t))\n",
    "        linebreak()\n",
    "        \n",
    "        t = time()\n",
    "        score = bleu_score(sess, model, data, _BUCKETS, fr_rev_vocab)\n",
    "        print(\"Evaluating BLEU score took %.6fs\" %(time() - t))\n",
    "        linebreak()\n",
    "        \n",
    "        print(\"BLEU Score: %.6f\" %(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test neural translation model.\n",
      "--------------------------------------------------\n",
      "Initializing Model took 27.370849s\n",
      "--------------------------------------------------\n",
      "Loading Model took 13.747781s\n",
      "--------------------------------------------------\n",
      "Evaluating BLEU score took 193.446001s\n",
      "--------------------------------------------------\n",
      "BLEU Score: 0.654400\n"
     ]
    }
   ],
   "source": [
    "test_model(newstest2013, CKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 337, 1038, 1284]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(len, newstest2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
